{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b73906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_spa_ingredients = {\n",
    "    \"Tomate\": [\"tomato\", \"roma tomato\", \"cherry tomato\", \"tomatillo\"],\n",
    "    \"Cebolla\": [\"onion\", \"red onion\", \"white onion\", \"yellow onion\", \"shallot\", \"green onion\", \"spring onion\", \"scallion\"],\n",
    "    \"Patata\": [\"potato\", \"new potato\", \"russet\", \"yukon gold\", \"baking potato\"],\n",
    "    \"Lechuga/Endivia\": [\"lettuce\", \"iceberg\", \"romaine\", \"butterhead\", \"bibb\", \"cos\", \"endive\", \"escarole\"],\n",
    "    \"Zanahoria\": [\"carrot\", \"baby carrot\"],\n",
    "    \"Calabacines\": [\"zucchini\", \"courgette\", \"summer squash\"],\n",
    "    \"Pepino\": [\"cucumber\", \"english cucumber\", \"kirby\"],\n",
    "    \"Champiñones\": [\"mushroom\", \"button mushroom\", \"cremini\", \"portobello\", \"shiitake\", \"oyster mushroom\", \"chanterelle\", \"porcini\"],\n",
    "    \"Brocoli\": [\"broccoli\", \"broccolini\"],\n",
    "    \"Coliflor\": [\"cauliflower\"],\n",
    "\n",
    "    \"Leche\": [\"milk\", \"whole milk\", \"skim milk\", \"2% milk\", \"evaporated milk\"],\n",
    "    \"Huevos\": [\"egg\", \"eggs\"],\n",
    "    \"Yogur\": [\"yogurt\", \"greek yogurt\", \"yoghurt\"],\n",
    "    \"Queso\": [\"cheese\", \"cheddar\", \"mozzarella\", \"parmesan\", \"feta\", \"gouda\", \"goat cheese\", \"blue cheese\", \"ricotta\", \"cream cheese\", \"swiss\"],\n",
    "    \"Mantequilla\": [\"butter\", \"unsalted butter\", \"salted butter\", \"ghee\"],\n",
    "\n",
    "    \"Merluza\": [\"hake\"],\n",
    "    \"Gambas/Langostinos\": [\"shrimp\", \"prawn\", \"prawns\", \"king prawn\"],\n",
    "    \"Mix de marisco/molusco\": [\"seafood mix\", \"mixed seafood\", \"clams\", \"mussels\", \"oysters\", \"scallops\", \"squid\", \"calamari\", \"octopus\"],\n",
    "    \"Lubina\": [\"sea bass\", \"seabass\", \"branzino\", \"european seabass\"],\n",
    "    \"Salmón\": [\"salmon\"],\n",
    "\n",
    "    \"Plátano\": [\"banana\", \"plantain\"],\n",
    "    \"Aguacate\": [\"avocado\"],\n",
    "    \"Sandía\": [\"watermelon\"],\n",
    "    \"Limón\": [\"lemon\"],\n",
    "    \"Manzana\": [\"apple\", \"granny smith\", \"gala apple\", \"fuji apple\"],\n",
    "\n",
    "    \"Carne pollo\": [\"chicken\", \"chicken breast\", \"chicken thigh\", \"chicken leg\", \"rotisserie chicken\", \"ground chicken\"],\n",
    "    \"Carne cerdo\": [\"pork\", \"pork loin\", \"pork chop\", \"pork shoulder\", \"ground pork\", \"bacon\"],\n",
    "    \"Carne vacuno\": [\"beef\", \"steak\", \"ground beef\", \"sirloin\", \"ribeye\", \"chuck\", \"brisket\"],\n",
    "    \"Salchichas\": [\"sausage\", \"sausages\", \"hot dog\", \"frankfurter\", \"chorizo\", \"kielbasa\", \"bratwurst\"],\n",
    "    \"Carne pavo\": [\"turkey\", \"ground turkey\", \"turkey breast\", \"turkey mince\"],\n",
    "}\n",
    "\n",
    "spa_spa_ingredients = {\n",
    "    \"Tomate\": [\"tomate\", \"jitomate\"],\n",
    "    \"Cebolla\": [\"cebolla\", \"cebolleta\"],\n",
    "    \"Patata\": [\"patata\", \"papa\"],\n",
    "    \"Lechuga/Endivia\": [\"lechuga\", \"endivia\", \"escarola\"],\n",
    "    \"Zanahoria\": [\"zanahoria\"],\n",
    "    \"Calabacines\": [\"calabacin\", \"calabacines\", \"zucchini\"],\n",
    "    \"Pepino\": [\"pepino\"],\n",
    "    \"Champiñones\": [\"champiñon\", \"champiñones\", \"seta\", \"hongos\", \"portobello\", \"shiitake\"],\n",
    "    \"Brocoli\": [\"brocoli\"],\n",
    "    \"Coliflor\": [\"coliflor\"],\n",
    "    \"Leche\": [\"leche\"],\n",
    "    \"Huevos\": [\"huevo\", \"huevos\"],\n",
    "    \"Yogur\": [\"yogur\"],\n",
    "    \"Queso\": [\"queso\"],\n",
    "    \"Mantequilla\": [\"mantequilla\", \"ghee\"],\n",
    "    \"Merluza\": [\"merluza\"],\n",
    "    \"Gambas/Langostinos\": [\"gamba\", \"gambas\", \"langostino\", \"langostinos\", \"camarón\", \"camaron\", \"camarones\"],\n",
    "    \"Mix de marisco/molusco\": [\"marisco\", \"molusco\", \"almeja\", \"mejillon\", \"mejillón\", \"ostras\", \"calamar\", \"pulpo\"],\n",
    "    \"Lubina\": [\"lubina\"],\n",
    "    \"Salmón\": [\"salmon\", \"salmón\"],\n",
    "    \"Plátano\": [\"platano\", \"plátano\", \"banana\", \"banano\"],\n",
    "    \"Aguacate\": [\"aguacate\", \"palta\"],\n",
    "    \"Sandía\": [\"sandia\", \"sandía\"],\n",
    "    \"Limón\": [\"limon\", \"limón\"],\n",
    "    \"Manzana\": [\"manzana\"],\n",
    "    \"Carne pollo\": [\"pollo\"],\n",
    "    \"Carne cerdo\": [\"cerdo\"],\n",
    "    \"Carne vacuno\": [\"vacuno\", \"ternera\", \"res\"],\n",
    "    \"Salchichas\": [\"salchicha\", \"salchichas\"],\n",
    "    \"Carne pavo\": [\"pavo\"],\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730dc32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast, re, unicodedata, torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers\n",
    "# ---------------------------\n",
    "def strip_accents(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFD\", s)\n",
    "    s = \"\".join(ch for ch in s if unicodedata.category(ch) != \"Mn\")\n",
    "    return unicodedata.normalize(\"NFKC\", s)\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    s = strip_accents((s or \"\").strip().lower())\n",
    "    s = re.sub(r\"[^a-z0-9\\s\\-\\/\\+]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Parse NER strings -> lists (keep phrases)\n",
    "# ---------------------------\n",
    "recipe_dataset[\"NER\"] = recipe_dataset[\"NER\"].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "# 2) Normalize full phrases (do NOT split into words)\n",
    "recipe_dataset[\"NER_terms\"] = recipe_dataset[\"NER\"].apply(\n",
    "    lambda phrases: [norm(p) for p in phrases if p]\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Build reverse indices for rule mapping\n",
    "# ---------------------------\n",
    "keywords_eng = {norm(kw): cls for cls, kws in en_spa_ingredients.items() for kw in kws}\n",
    "keywords_spa = {norm(kw): cls for cls, kws in spa_spa_ingredients.items() for kw in kws}\n",
    "\n",
    "def rule_map_en(term: str):\n",
    "    t = norm(term)\n",
    "    if not t: return None\n",
    "    if t in keywords_eng:\n",
    "        return keywords_eng[t]\n",
    "    for kw, cls in keywords_eng.items():\n",
    "        if kw and kw in t:\n",
    "            return cls\n",
    "    return None\n",
    "\n",
    "def rule_map_es(term: str):\n",
    "    t = norm(term)\n",
    "    if not t: return None\n",
    "    if t in keywords_spa:\n",
    "        return keywords_spa[t]\n",
    "    for kw, cls in keywords_spa.items():\n",
    "        if kw and kw in t:\n",
    "            return cls\n",
    "    return None\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Collect unique phrase terms\n",
    "# ---------------------------\n",
    "unique_terms = sorted({t for terms in recipe_dataset[\"NER_terms\"] for t in terms if t})\n",
    "print(\"Unique phrase-level terms:\", len(unique_terms))\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Rule-map English phrases; collect unknowns to translate\n",
    "# ---------------------------\n",
    "eng_rule_map = {}\n",
    "unknown_en = []\n",
    "for term in unique_terms:\n",
    "    cls = rule_map_en(term)\n",
    "    if cls:\n",
    "        eng_rule_map[term] = cls\n",
    "    else:\n",
    "        unknown_en.append(term)\n",
    "\n",
    "print(\"Rule-mapped (EN):\", len(eng_rule_map), \" | To translate:\", len(unknown_en))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a4c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 6) Translate unknown phrases with Marian (GPU if available) + progress\n",
    "# ---------------------------\n",
    "mt_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "mt_tok = MarianTokenizer.from_pretrained(mt_name)\n",
    "mt_model = MarianMTModel.from_pretrained(mt_name).to(device)\n",
    "mt_model.eval()\n",
    "\n",
    "def translate_batch(texts, src_max_len=256, max_new_tokens=128):\n",
    "    if not texts:\n",
    "        return []\n",
    "    with torch.inference_mode():\n",
    "        inputs = mt_tok(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=src_max_len,\n",
    "        ).to(device)\n",
    "        outputs = mt_model.generate(\n",
    "            **inputs,\n",
    "            num_beams=1,            # greedy = fastest\n",
    "            do_sample=False,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            use_cache=True,\n",
    "        )\n",
    "        return mt_tok.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "BATCH = 96 if device == \"cuda\" else 24\n",
    "en2es = {}\n",
    "total = len(unknown_en)\n",
    "for i in range(0, total, BATCH):\n",
    "    chunk = unknown_en[i:i+BATCH]\n",
    "    trans = translate_batch(chunk)\n",
    "    for src, tgt in zip(chunk, trans):\n",
    "        en2es[src] = tgt\n",
    "    print(f\"Progress: {min(i+BATCH, total)}/{total} translated\")\n",
    "\n",
    "print(\"Translated unknowns:\", len(en2es))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb07a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 7) Merge mappings (rule-based has priority)\n",
    "# ---------------------------\n",
    "full_mapping = {**en2es, **eng_rule_map}  # rule-based overrides MT on conflicts\n",
    "\n",
    "# ---------------------------\n",
    "# 8) Map back to dataframe (phrase-by-phrase)\n",
    "# ---------------------------\n",
    "def map_list_to_spanish(terms, mapping):\n",
    "    return [mapping.get(t, t) for t in (terms or [])]\n",
    "\n",
    "recipe_dataset[\"NER_terms_es\"] = recipe_dataset[\"NER_terms\"].apply(\n",
    "    lambda ts: map_list_to_spanish(ts, full_mapping)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddfc94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_dataset.to_parquet(\"recipes_dataset_translated.parquet\", engine=\"pyarrow\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d4557",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_dataset = pd.read_csv(r'..\\dataset\\Recipes dataset\\recipes_dataset.csv')\n",
    "# Data source is https://huggingface.co/datasets/mbien/recipe_nlg, not uploading because it's too large for Github"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
